{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "genAns.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kapJ5GTksWeU",
        "outputId": "5f1c9031-c405-4539-e773-5162f34f4de7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4vcHq2Tzv_V",
        "outputId": "bee50fef-1041-4dda-9c15-6a16439a3955"
      },
      "source": [
        "%cd \"gdrive/MyDrive/Colab Notebooks/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykUvSq3pCSwX",
        "outputId": "b4c92786-4304-41da-8bda-92a83fbcc413"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'=1.6.0'\n",
            " 1712396_AssignmentML\n",
            " 1712396_BT1_KiemTraHamSinhSo.ipynb\n",
            " 1712396_BT2_MNIST-Classification-pytorch.ipynb\n",
            " 1712396_BTL_GANforCIFAR_10.ipynb\n",
            " 1712396_BTL_VAEforCIFAR_10.ipynb\n",
            " beam.txt\n",
            " bottom-up-feat.ipynb\n",
            " data\n",
            " data1M300d.npz\n",
            " data.zip\n",
            " demo_faster_rcnn.ipynb\n",
            " demo.ipynb\n",
            " DoggyTask.ipynb\n",
            " eng-fra.txt\n",
            " en-vi.ipynb\n",
            " envi-model.avg-250000.tar.xz\n",
            " en.wiki.bpe.vs10000.model\n",
            " genAns.ipynb\n",
            " glove.6B.100d.txt\n",
            " glove.6B.200d.txt\n",
            " glove.6B.300d.txt\n",
            " glove.6B.50d.txt\n",
            " glove.6B.zip\n",
            " gnmt_en_vi_u512\n",
            " ImagePresentation.ipynb\n",
            " MNIST-cnn-pytorch.ipynb\n",
            " model_en_vi_epoch19_9.12.t7\n",
            " netflix_titles.csv.gsheet\n",
            " nmt\n",
            " NMT_icode.ipynb\n",
            " output2021-03-16.csv.gsheet\n",
            " t2t_export\n",
            " Task1WhattowatchonNetflix.ipynb\n",
            " tensorboard-logs\n",
            " test-2013-en-vi.tgz\n",
            " train_answer.txt\n",
            " train_en.txt\n",
            " train_question.txt\n",
            " train_vi.txt\n",
            " TranslateFrenchToEng.ipynb\n",
            " translate.ipynb\n",
            " TranslationEn-Vi.ipynb\n",
            " Translation.ipynb\n",
            " tst2013.en\n",
            " tst2013.vi\n",
            " Tut1-Pytorch.ipynb\n",
            " Tut2.ipynb\n",
            " Tut3.ipynb\n",
            " Tut4.ipynb\n",
            " Untitled\n",
            " Untitled0.ipynb\n",
            " Untitled1.ipynb\n",
            " Untitled2.ipynb\n",
            " Untitled3.ipynb\n",
            " validation_answer.txt\n",
            " validation_question.txt\n",
            " vie.txt\n",
            " vqa_raw_train_v1.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqiMDscqz4lB"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPAdTDfJ3G9z"
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "from collections import Counter, namedtuple\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOrjKQyCBDze",
        "outputId": "77d4e450-2d4c-4dc5-f52c-c7f85baa7ae0"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy[cuda102]\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (54.1.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.36.2)\n",
            "Requirement already satisfied: spacy[cuda102] in /usr/local/lib/python3.7/dist-packages (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (3.7.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (54.1.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (2.0.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (8.0.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (3.0.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (1.7.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (1.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (3.7.4.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (1.19.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (0.4.0)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (0.3.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (2.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (3.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (20.9)\n",
            "Requirement already satisfied: cupy-cuda102<9.0.0,>=5.0.0b4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda102]) (8.5.0)\n",
            "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda102<9.0.0,>=5.0.0b4->spacy[cuda102]) (0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->spacy[cuda102]) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[cuda102]) (2.4.7)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[cuda102]) (3.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda102]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda102]) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda102]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda102]) (3.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy[cuda102]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[cuda102]) (1.1.1)\n",
            "2021-03-20 10:41:05.247032: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7 MB 75 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.1.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2021-03-20 10:41:12.014870: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-lg==3.0.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl (778.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 778.8 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (54.1.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.7.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (20.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyH6nlxDBHHQ",
        "outputId": "a3e57ab8-ae01-4b0f-8834-9ddface07c67"
      },
      "source": [
        "nlp = spacy.load('en_core_web_lg')\n",
        "print(nlp.vocab.vectors_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-GzIUwwz_X-"
      },
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "PAD = 0\n",
        "BOS = 1\n",
        "EOS = 2\n",
        "UNK = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKWAahjA0XgR",
        "outputId": "c2977ae5-4c9d-488f-c6ea-5c35acb3f1b3"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('current_device={}'.format(torch.cuda.current_device()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current_device=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiuEuewm1VsO"
      },
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *av, **kav):\n",
        "        dict.__init__(self, *av, **kav)\n",
        "        self.__dict__ = self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZoUC4z_1dFe"
      },
      "source": [
        "class QuestionAnswerDataset(Dataset):\n",
        "    def __init__(self, src_path, tgt_path, src_vocab=None, tgt_vocab=None, max_vocab_size=50000, share_vocab=True):\n",
        "        \"\"\" Note: If src_vocab, tgt_vocab is not given, it will build both vocabs.\n",
        "            Args: \n",
        "            - src_path, tgt_path: text file with tokenized sentences.\n",
        "            - src_vocab, tgt_vocab: data structure is same as self.build_vocab().\n",
        "        \"\"\"\n",
        "        print('='*100)\n",
        "        print('Dataset preprocessing log:')\n",
        "        print('- Loading and tokenizing source sentences...')\n",
        "        self.src_sents = self.load_sents(src_path)\n",
        "        print('- Loading and tokenizing target sentences...')\n",
        "        self.tgt_sents = self.load_sents(tgt_path)\n",
        "        if src_vocab is None or tgt_vocab is None:\n",
        "            print('- Building source counter...')\n",
        "            self.src_counter = self.build_counter(self.src_sents)\n",
        "            print('- Building target counter...')\n",
        "            self.tgt_counter = self.build_counter(self.tgt_sents)\n",
        "            if share_vocab:\n",
        "                print('- Building source vocabulary...')\n",
        "                self.src_vocab = self.build_vocab(self.src_counter + self.tgt_counter, max_vocab_size)\n",
        "                print('- Building target vocabulary...')\n",
        "                self.tgt_vocab = self.src_vocab\n",
        "            else:\n",
        "                print('- Building source vocabulary...')\n",
        "                self.src_vocab = self.build_vocab(self.src_counter, max_vocab_size)\n",
        "                print('- Building target vocabulary...')\n",
        "                self.tgt_vocab = self.build_vocab(self.tgt_counter, max_vocab_size)\n",
        "        else:\n",
        "            self.src_vocab = src_vocab\n",
        "            self.tgt_vocab = tgt_vocab\n",
        "            share_vocab = src_vocab == tgt_vocab\n",
        "        print('='*100)\n",
        "        print('Dataset Info:')\n",
        "        print('- Number of source sentences: {}'.format(len(self.src_sents)))\n",
        "        print('- Number of target sentences: {}'.format(len(self.tgt_sents)))\n",
        "        print('- Source vocabulary size: {}'.format(len(self.src_vocab.token2id)))\n",
        "        print('- Target vocabulary size: {}'.format(len(self.tgt_vocab.token2id)))\n",
        "        print('- Shared vocabulary: {}'.format(share_vocab))\n",
        "        print('='*100 + '\\n')\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.src_sents)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        src_sent = self.src_sents[index]\n",
        "        tgt_sent = self.tgt_sents[index]\n",
        "        src_seq = self.tokens2ids(src_sent, self.src_vocab.token2id, append_BOS=False, append_EOS=True)\n",
        "        tgt_seq = self.tokens2ids(tgt_sent, self.tgt_vocab.token2id, append_BOS=False, append_EOS=True)\n",
        "\n",
        "        return src_sent, tgt_sent, src_seq, tgt_seq\n",
        "    \n",
        "    def load_sents(self, file_path):\n",
        "        sents = []\n",
        "        with codecs.open(file_path) as f:\n",
        "            for sent in tqdm(f.readlines()):\n",
        "                tokens = [token for token in sent.split()]\n",
        "                sents.append(tokens)\n",
        "        return sents\n",
        "    \n",
        "    def build_counter(self, sents):\n",
        "        counter = Counter()\n",
        "        for sent in tqdm(sents):\n",
        "            counter.update(sent)\n",
        "        return counter\n",
        "    \n",
        "    def build_vocab(self, counter, max_vocab_size):\n",
        "        vocab = AttrDict()\n",
        "        vocab.token2id = {'<PAD>': PAD, '<BOS>': BOS, '<EOS>': EOS, '<UNK>': UNK}\n",
        "        vocab.token2id.update({token: _id+4 for _id, (token, count) in tqdm(enumerate(counter.most_common(max_vocab_size)))})\n",
        "        vocab.id2token = {v:k for k,v in tqdm(vocab.token2id.items())}    \n",
        "        return vocab\n",
        "    \n",
        "    def tokens2ids(self, tokens, token2id, append_BOS=True, append_EOS=True):\n",
        "        seq = []\n",
        "        if append_BOS: seq.append(BOS)\n",
        "        seq.extend([token2id.get(token, UNK) for token in tokens])\n",
        "        if append_EOS: seq.append(EOS)\n",
        "        return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctpkL6lc2P7h"
      },
      "source": [
        "def collate_fn(data):\n",
        "    \"\"\"\n",
        "    Creates mini-batch tensors from (src_sent, tgt_sent, src_seq, tgt_seq).\n",
        "    We should build a custom collate_fn rather than using default collate_fn,\n",
        "    because merging sequences (including padding) is not supported in default.\n",
        "    Seqeuences are padded to the maximum length of mini-batch sequences (dynamic padding).\n",
        "    \n",
        "    Args:\n",
        "        data: list of tuple (src_sents, tgt_sents, src_seqs, tgt_seqs)\n",
        "        - src_sents, tgt_sents: batch of original tokenized sentences\n",
        "        - src_seqs, tgt_seqs: batch of original tokenized sentence ids\n",
        "    Returns:\n",
        "        - src_sents, tgt_sents (tuple): batch of original tokenized sentences\n",
        "        - src_seqs, tgt_seqs (variable): (max_src_len, batch_size)\n",
        "        - src_lens, tgt_lens (tensor): (batch_size)\n",
        "       \n",
        "    \"\"\"\n",
        "    def _pad_sequences(seqs):\n",
        "        lens = [len(seq) for seq in seqs]\n",
        "        padded_seqs = torch.zeros(len(seqs), max(lens)).long()\n",
        "        for i, seq in enumerate(seqs):\n",
        "            end = lens[i]\n",
        "            padded_seqs[i, :end] = torch.LongTensor(seq[:end])\n",
        "        return padded_seqs, lens\n",
        "\n",
        "    # Sort a list by *source* sequence length (descending order) to use `pack_padded_sequence`.\n",
        "    # The *target* sequence is not sorted <-- It's ok, cause `pack_padded_sequence` only takes\n",
        "    # *source* sequence, which is in the EncoderRNN\n",
        "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "\n",
        "    # Seperate source and target sequences.\n",
        "    src_sents, tgt_sents, src_seqs, tgt_seqs = zip(*data)\n",
        "    \n",
        "    # Merge sequences (from tuple of 1D tensor to 2D tensor)\n",
        "    src_seqs, src_lens = _pad_sequences(src_seqs)\n",
        "    tgt_seqs, tgt_lens = _pad_sequences(tgt_seqs)\n",
        "    \n",
        "    # (batch, seq_len) => (seq_len, batch)\n",
        "    src_seqs = src_seqs.transpose(0,1)\n",
        "    tgt_seqs = tgt_seqs.transpose(0,1)\n",
        "\n",
        "    return src_sents, tgt_sents, src_seqs, tgt_seqs, src_lens, tgt_lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR42VbCt9XNf"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, embedding=None, rnn_type='LSTM', hidden_size=128, num_layers=1, dropout=0.3, bidirectional=True):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        self.hidden_size = hidden_size // self.num_directions\n",
        "        \n",
        "        self.embedding = embedding\n",
        "        self.word_vec_size = self.embedding.embedding_dim\n",
        "        \n",
        "        self.rnn_type = rnn_type\n",
        "        self.rnn = getattr(nn, self.rnn_type)(\n",
        "                           input_size=self.word_vec_size,\n",
        "                           hidden_size=self.hidden_size,\n",
        "                           num_layers=self.num_layers,\n",
        "                           dropout=self.dropout, \n",
        "                           bidirectional=self.bidirectional)\n",
        "        \n",
        "    def forward(self, src_seqs, src_lens, hidden=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            - src_seqs: (max_src_len, batch_size)\n",
        "            - src_lens: (batch_size)\n",
        "        Returns:\n",
        "            - outputs: (max_src_len, batch_size, hidden_size * num_directions)\n",
        "            - hidden : (num_layers, batch_size, hidden_size * num_directions)\n",
        "        \"\"\"\n",
        "        \n",
        "        # (max_src_len, batch_size) => (max_src_len, batch_size, word_vec_size)\n",
        "        emb = self.embedding(src_seqs)\n",
        "\n",
        "        # packed_emb:\n",
        "        # - data: (sum(batch_sizes), word_vec_size)\n",
        "        # - batch_sizes: list of batch sizes\n",
        "        packed_emb = nn.utils.rnn.pack_padded_sequence(emb, src_lens)\n",
        "\n",
        "        # rnn(gru) returns:\n",
        "        # - packed_outputs: shape same as packed_emb\n",
        "        # - hidden: (num_layers * num_directions, batch_size, hidden_size) \n",
        "        packed_outputs, hidden = self.rnn(packed_emb, hidden)\n",
        "\n",
        "        # outputs: (max_src_len, batch_size, hidden_size * num_directions)\n",
        "        # output_lens == src_lensˇ\n",
        "        outputs, output_lens =  nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "        \n",
        "        if self.bidirectional:\n",
        "            # (num_layers * num_directions, batch_size, hidden_size) \n",
        "            # => (num_layers, batch_size, hidden_size * num_directions)\n",
        "            hidden = self._cat_directions(hidden)\n",
        "        \n",
        "        return outputs, hidden\n",
        "    \n",
        "    def _cat_directions(self, hidden):\n",
        "        \"\"\" If the encoder is bidirectional, do the following transformation.\n",
        "            Ref: https://github.com/IBM/pytorch-seq2seq/blob/master/seq2seq/models/DecoderRNN.py#L176\n",
        "            -----------------------------------------------------------\n",
        "            In: (num_layers * num_directions, batch_size, hidden_size)\n",
        "            (ex: num_layers=2, num_directions=2)\n",
        "\n",
        "            layer 1: forward__hidden(1)\n",
        "            layer 1: backward_hidden(1)\n",
        "            layer 2: forward__hidden(2)\n",
        "            layer 2: backward_hidden(2)\n",
        "\n",
        "            -----------------------------------------------------------\n",
        "            Out: (num_layers, batch_size, hidden_size * num_directions)\n",
        "\n",
        "            layer 1: forward__hidden(1) backward_hidden(1)\n",
        "            layer 2: forward__hidden(2) backward_hidden(2)\n",
        "        \"\"\"\n",
        "        def _cat(h):\n",
        "            return torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
        "            \n",
        "        if isinstance(hidden, tuple):\n",
        "            # LSTM hidden contains a tuple (hidden state, cell state)\n",
        "            hidden = tuple([_cat(h) for h in hidden])\n",
        "        else:\n",
        "            # GRU hidden\n",
        "            hidden = _cat(hidden)\n",
        "            \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpxuBWFa9J_0"
      },
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, encoder, embedding=None, attention=True, bias=True, tie_embeddings=False, dropout=0.3):\n",
        "        \"\"\" General attention in `Effective Approaches to Attention-based Neural Machine Translation`\n",
        "            Ref: https://arxiv.org/abs/1508.04025\n",
        "            \n",
        "            Share input and output embeddings:\n",
        "            Ref:\n",
        "                - \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "                   https://arxiv.org/abs/1608.05859\n",
        "                - \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "                   https://arxiv.org/abs/1611.01462\n",
        "        \"\"\"\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "        \n",
        "        self.hidden_size = encoder.hidden_size * encoder.num_directions\n",
        "        self.num_layers = encoder.num_layers\n",
        "        self.dropout = dropout\n",
        "        self.embedding = embedding\n",
        "        self.attention = attention\n",
        "        self.tie_embeddings = tie_embeddings\n",
        "        \n",
        "        self.vocab_size = self.embedding.num_embeddings\n",
        "        self.word_vec_size = self.embedding.embedding_dim\n",
        "        \n",
        "        self.rnn_type = encoder.rnn_type\n",
        "        self.rnn = getattr(nn, self.rnn_type)(\n",
        "                            input_size=self.word_vec_size,\n",
        "                            hidden_size=self.hidden_size,\n",
        "                            num_layers=self.num_layers,\n",
        "                            dropout=self.dropout) \n",
        "        if self.attention:\n",
        "            self.W_a = nn.Linear(encoder.hidden_size * encoder.num_directions,\n",
        "                                 self.hidden_size, bias=bias)\n",
        "            self.W_c = nn.Linear(encoder.hidden_size * encoder.num_directions + self.hidden_size, \n",
        "                                 self.hidden_size, bias=bias)\n",
        "        \n",
        "        if self.tie_embeddings:\n",
        "            self.W_proj = nn.Linear(self.hidden_size, self.word_vec_size, bias=bias)\n",
        "           \n",
        "            self.W_s = nn.Linear(self.word_vec_size, self.vocab_size, bias=bias)\n",
        "            \n",
        "            self.W_s.weight = self.embedding.weight\n",
        "           \n",
        "        else:\n",
        "            self.W_s = nn.Linear(self.hidden_size, self.vocab_size, bias=bias)\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self, input_seq, decoder_hidden, encoder_outputs, src_lens):\n",
        "        \"\"\" Args:\n",
        "            - input_seq      : (batch_size)\n",
        "            - decoder_hidden : (t=0) last encoder hidden state (num_layers * num_directions, batch_size, hidden_size) \n",
        "                               (t>0) previous decoder hidden state (num_layers, batch_size, hidden_size)\n",
        "            - encoder_outputs: (max_src_len, batch_size, hidden_size * num_directions)\n",
        "        \n",
        "            Returns:\n",
        "            - output           : (batch_size, vocab_size)\n",
        "            - decoder_hidden   : (num_layers, batch_size, hidden_size)\n",
        "            - attention_weights: (batch_size, max_src_len)\n",
        "        \"\"\"        \n",
        "        # (batch_size) => (seq_len=1, batch_size)\n",
        "        input_seq = input_seq.unsqueeze(0)\n",
        "        # (seq_len=1, batch_size) => (seq_len=1, batch_size, word_vec_size) \n",
        "        emb = self.embedding(input_seq)\n",
        "        \n",
        "        # rnn returns:\n",
        "        # - decoder_output: (seq_len=1, batch_size, hidden_size)\n",
        "        # - decoder_hidden: (num_layers, batch_size, hidden_size)\n",
        "        decoder_output, decoder_hidden = self.rnn(emb, decoder_hidden)\n",
        "\n",
        "        # (seq_len=1, batch_size, hidden_size) => (batch_size, seq_len=1, hidden_size)\n",
        "        decoder_output = decoder_output.transpose(0,1)\n",
        "        \n",
        "        \"\"\" \n",
        "        ------------------------------------------------------------------------------------------\n",
        "        Notes of computing attention scores\n",
        "        ------------------------------------------------------------------------------------------\n",
        "        # For-loop version:\n",
        "\n",
        "        max_src_len = encoder_outputs.size(0)\n",
        "        batch_size = encoder_outputs.size(1)\n",
        "        attention_scores = Variable(torch.zeros(batch_size, max_src_len))\n",
        "\n",
        "        # For every batch, every time step of encoder's hidden state, calculate attention score.\n",
        "        for b in range(batch_size):\n",
        "            for t in range(max_src_len):\n",
        "                # Loung. eq(8) -- general form content-based attention:\n",
        "                attention_scores[b,t] = decoder_output[b].dot(attention.W_a(encoder_outputs[t,b]))\n",
        "\n",
        "        ------------------------------------------------------------------------------------------\n",
        "        # Vectorized version:\n",
        "\n",
        "        1. decoder_output: (batch_size, seq_len=1, hidden_size)\n",
        "        2. encoder_outputs: (max_src_len, batch_size, hidden_size * num_directions)\n",
        "        3. W_a(encoder_outputs): (max_src_len, batch_size, hidden_size)\n",
        "                        .transpose(0,1)  : (batch_size, max_src_len, hidden_size) \n",
        "                        .transpose(1,2)  : (batch_size, hidden_size, max_src_len)\n",
        "        4. attention_scores: \n",
        "                        (batch_size, seq_len=1, hidden_size) * (batch_size, hidden_size, max_src_len) \n",
        "                        => (batch_size, seq_len=1, max_src_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.attention:\n",
        "            # attention_scores: (batch_size, seq_len=1, max_src_len)\n",
        "            attention_scores = torch.bmm(decoder_output, self.W_a(encoder_outputs).transpose(0,1).transpose(1,2))\n",
        "\n",
        "            # attention_mask: (batch_size, seq_len=1, max_src_len)\n",
        "            attention_mask = sequence_mask(src_lens).unsqueeze(1)\n",
        "\n",
        "            # Fills elements of tensor with `-float('inf')` where `mask` is 1.\n",
        "            attention_scores.data.masked_fill_(torch.logical_not(attention_mask.data), -float('inf'))\n",
        "\n",
        "            # attention_weights: (batch_size, seq_len=1, max_src_len) => (batch_size, max_src_len) for `F.softmax` \n",
        "            # => (batch_size, seq_len=1, max_src_len)\n",
        "            try: # torch 0.3.x\n",
        "                attention_weights = F.softmax(attention_scores.squeeze(1), dim=1).unsqueeze(1)\n",
        "            except:\n",
        "                attention_weights = F.softmax(attention_scores.squeeze(1)).unsqueeze(1)\n",
        "\n",
        "            # context_vector:\n",
        "            # (batch_size, seq_len=1, max_src_len) * (batch_size, max_src_len, encoder_hidden_size * num_directions)\n",
        "            # => (batch_size, seq_len=1, encoder_hidden_size * num_directions)\n",
        "            context_vector = torch.bmm(attention_weights, encoder_outputs.transpose(0,1))\n",
        "\n",
        "            # concat_input: (batch_size, seq_len=1, encoder_hidden_size * num_directions + decoder_hidden_size)\n",
        "            concat_input = torch.cat([context_vector, decoder_output], -1)\n",
        "\n",
        "            # (batch_size, seq_len=1, encoder_hidden_size * num_directions + decoder_hidden_size) => (batch_size, seq_len=1, decoder_hidden_size)\n",
        "            concat_output = F.tanh(self.W_c(concat_input))\n",
        "            \n",
        "            # Prepare returns:\n",
        "            # (batch_size, seq_len=1, max_src_len) => (batch_size, max_src_len)\n",
        "            attention_weights = attention_weights.squeeze(1)\n",
        "        else:\n",
        "            attention_weights = None\n",
        "            concat_output = decoder_output\n",
        "        \n",
        "        # If input and output embeddings are tied,\n",
        "        # project `decoder_hidden_size` to `word_vec_size`.\n",
        "        if self.tie_embeddings:\n",
        "            output = self.W_s(self.W_proj(concat_output))\n",
        "        else:\n",
        "            # (batch_size, seq_len=1, decoder_hidden_size) => (batch_size, seq_len=1, vocab_size)\n",
        "            output = self.W_s(concat_output)    \n",
        "        \n",
        "        # Prepare returns:\n",
        "        # (batch_size, seq_len=1, vocab_size) => (batch_size, vocab_size)\n",
        "        output = output.squeeze(1)\n",
        "        \n",
        "        del src_lens\n",
        "        \n",
        "        return output, decoder_hidden, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhMkr0GK9K7n"
      },
      "source": [
        "def load_spacy_glove_embedding(spacy_nlp, vocab):\n",
        "    \n",
        "    vocab_size = len(vocab.token2id)\n",
        "    word_vec_size = spacy_nlp.vocab.vectors_length\n",
        "    embedding = np.zeros((vocab_size, word_vec_size))\n",
        "    unk_count = 0\n",
        "    \n",
        "    print('='*100)\n",
        "    print('Loading spacy glove embedding:')\n",
        "    print('- Vocabulary size: {}'.format(vocab_size))\n",
        "    print('- Word vector size: {}'.format(word_vec_size))\n",
        "    \n",
        "    for token, index in tqdm(vocab.token2id.items()):\n",
        "        if token == vocab.id2token[PAD]: \n",
        "            continue\n",
        "        elif token in [vocab.id2token[BOS], vocab.id2token[EOS], vocab.id2token[UNK]]: \n",
        "            vector = np.random.rand(word_vec_size,)\n",
        "        elif spacy_nlp.vocab[token].has_vector: \n",
        "            vector = spacy_nlp.vocab[token].vector\n",
        "        else:\n",
        "            vector = embedding[UNK] \n",
        "            unk_count += 1\n",
        "            \n",
        "        embedding[index] = vector\n",
        "        \n",
        "    print('- Unknown word count: {}'.format(unk_count))\n",
        "    print('='*100 + '\\n')\n",
        "        \n",
        "    return torch.from_numpy(embedding).float()\n",
        "\n",
        "def sequence_mask(sequence_length, max_len=None):\n",
        "    \"\"\"\n",
        "    Caution: Input and Return are VARIABLE.\n",
        "    \"\"\"\n",
        "    if max_len is None:\n",
        "        max_len = sequence_length.data.max()\n",
        "    batch_size = sequence_length.size(0)\n",
        "    seq_range = torch.arange(0, max_len).long()\n",
        "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
        "    seq_range_expand = Variable(seq_range_expand)\n",
        "    if sequence_length.is_cuda:\n",
        "        seq_range_expand = seq_range_expand.cuda()\n",
        "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
        "                         .expand_as(seq_range_expand))\n",
        "    mask = seq_range_expand < seq_length_expand\n",
        "    \n",
        "    return mask\n",
        "\n",
        "def masked_cross_entropy(logits, target, length):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        logits: A Variable containing a FloatTensor of size\n",
        "            (batch, max_len, num_classes) which contains the\n",
        "            unnormalized probability for each class.\n",
        "        target: A Variable containing a LongTensor of size\n",
        "            (batch, max_len) which contains the index of the true\n",
        "            class for each corresponding step.\n",
        "        length: A Variable containing a LongTensor of size (batch,)\n",
        "            which contains the length of each data in a batch.\n",
        "    Returns:\n",
        "        loss: An average loss value masked by the length.\n",
        "        \n",
        "    The code is same as:\n",
        "    \n",
        "    weight = torch.ones(tgt_vocab_size)\n",
        "    weight[padding_idx] = 0\n",
        "    criterion = nn.CrossEntropyLoss(weight.cuda(), size_average)\n",
        "    loss = criterion(logits_flat, losses_flat)\n",
        "    \"\"\"\n",
        "    # logits_flat: (batch * max_len, num_classes)\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    # log_probs_flat: (batch * max_len, num_classes)\n",
        "    log_probs_flat = F.log_softmax(logits_flat)\n",
        "    # target_flat: (batch * max_len, 1)\n",
        "    target_flat = target.view(-1, 1)\n",
        "    # losses_flat: (batch * max_len, 1)\n",
        "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
        "    # losses: (batch, max_len)\n",
        "    losses = losses_flat.view(*target.size())\n",
        "    # mask: (batch, max_len)\n",
        "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
        "    # Note: mask need to bed casted to float!\n",
        "    losses = losses * mask.float()\n",
        "    loss = losses.sum() / mask.float().sum()\n",
        "    \n",
        "    # (batch_size * max_tgt_len,)\n",
        "    pred_flat = log_probs_flat.max(1)[1]\n",
        "    # (batch_size * max_tgt_len,) => (batch_size, max_tgt_len) => (max_tgt_len, batch_size)\n",
        "    pred_seqs = pred_flat.view(*target.size()).transpose(0,1).contiguous()\n",
        "    # (batch_size, max_len) => (batch_size * max_tgt_len,)\n",
        "    mask_flat = mask.view(-1)\n",
        "    \n",
        "    # `.float()` IS VERY IMPORTANT !!!\n",
        "    # https://discuss.pytorch.org/t/batch-size-and-validation-accuracy/4066/3\n",
        "    num_corrects = int(pred_flat.eq(target_flat.squeeze(1)).masked_select(mask_flat).float().data.sum())\n",
        "    num_words = length.data.sum()\n",
        "\n",
        "    return loss, pred_seqs, num_corrects, num_words\n",
        "\n",
        "def load_checkpoint(checkpoint_path):\n",
        "    # It's weird that if `map_location` is not given, it will be extremely slow.\n",
        "    return torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
        "\n",
        "def save_checkpoint(opts, experiment_name, encoder, decoder, encoder_optim, decoder_optim,\n",
        "                    total_accuracy, total_loss, global_step):\n",
        "    checkpoint = {\n",
        "        'opts': opts,\n",
        "        'global_step': global_step,\n",
        "        'encoder_state_dict': encoder.state_dict(),\n",
        "        'decoder_state_dict': decoder.state_dict(),\n",
        "        'encoder_optim_state_dict': encoder_optim.state_dict(),\n",
        "        'decoder_optim_state_dict': decoder_optim.state_dict()\n",
        "    }\n",
        "    \n",
        "    checkpoint_path = 'checkpoints/%s_acc_%.2f_loss_%.2f_step_%d.pt' % (experiment_name, total_accuracy, total_loss, global_step)\n",
        "    \n",
        "    directory, filename = os.path.split(os.path.abspath(checkpoint_path))\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    \n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    \n",
        "    return checkpoint_path\n",
        "\n",
        "def variable2numpy(var):\n",
        "    \"\"\" For tensorboard visualization \"\"\"\n",
        "    return var.data.cpu().numpy()\n",
        "\n",
        "def write_to_tensorboard(writer, global_step, total_loss, total_corrects, total_words, total_accuracy,\n",
        "                         encoder_grad_norm, decoder_grad_norm, clipped_encoder_grad_norm, clipped_decoder_grad_norm,\n",
        "                         encoder, decoder, gpu_memory_usage=None):\n",
        "    # scalars\n",
        "    if gpu_memory_usage is not None:\n",
        "        writer.add_scalar('curr_gpu_memory_usage', gpu_memory_usage['curr'], global_step)\n",
        "        writer.add_scalar('diff_gpu_memory_usage', gpu_memory_usage['diff'], global_step)\n",
        "        \n",
        "    writer.add_scalar('total_loss', total_loss, global_step)\n",
        "    writer.add_scalar('total_accuracy', total_accuracy, global_step)\n",
        "    writer.add_scalar('total_corrects', total_corrects, global_step)\n",
        "    writer.add_scalar('total_words', total_words, global_step)\n",
        "    writer.add_scalar('encoder_grad_norm', encoder_grad_norm, global_step)\n",
        "    writer.add_scalar('decoder_grad_norm', decoder_grad_norm, global_step)\n",
        "    writer.add_scalar('clipped_encoder_grad_norm', clipped_encoder_grad_norm, global_step)\n",
        "    writer.add_scalar('clipped_decoder_grad_norm', clipped_decoder_grad_norm, global_step)\n",
        "    \n",
        "    # histogram\n",
        "    for name, param in encoder.named_parameters():\n",
        "        name = name.replace('.', '/')\n",
        "        writer.add_histogram('encoder/{}'.format(name), variable2numpy(param), global_step, bins='doane')\n",
        "        if param.grad is not None:\n",
        "            writer.add_histogram('encoder/{}/grad'.format(name), variable2numpy(param.grad), global_step, bins='doane')\n",
        "\n",
        "    for name, param in decoder.named_parameters():\n",
        "        name = name.replace('.', '/')\n",
        "        writer.add_histogram('decoder/{}'.format(name), variable2numpy(param), global_step, bins='doane')\n",
        "        if param.grad is not None:\n",
        "            writer.add_histogram('decoder/{}/grad'.format(name), variable2numpy(param.grad), global_step, bins='doane')\n",
        "            \n",
        "def detach_hidden(hidden):\n",
        "    \"\"\" Wraps hidden states in new Variables, to detach them from their history. Prevent OOM.\n",
        "        After detach, the hidden's requires_grad=Fasle and grad_fn=None.\n",
        "    Issues:\n",
        "    - Memory leak problem in LSTM and RNN: https://github.com/pytorch/pytorch/issues/2198\n",
        "    - https://github.com/pytorch/examples/blob/master/word_language_model/main.py\n",
        "    - https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226\n",
        "    - https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426\n",
        "    - \n",
        "    \"\"\"\n",
        "    if type(hidden) == Variable:\n",
        "        hidden.detach_() # same as creating a new variable.\n",
        "    else:\n",
        "        for h in hidden: h.detach_()\n",
        "\n",
        "def get_gpu_memory_usage(device_id):\n",
        "    \"\"\"Get the current gpu usage. \"\"\"\n",
        "    result = subprocess.check_output(\n",
        "        [\n",
        "            'nvidia-smi', '--query-gpu=memory.used',\n",
        "            '--format=csv,nounits,noheader'\n",
        "        ], encoding='utf-8')\n",
        "    # Convert lines into a dictionary\n",
        "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
        "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
        "    return gpu_memory_map[device_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17TR8ig4EsVJ"
      },
      "source": [
        "def compute_grad_norm(parameters, norm_type=2):\n",
        "    \"\"\" Ref: http://pytorch.org/docs/0.3.0/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm\n",
        "    \"\"\"\n",
        "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
        "    norm_type = float(norm_type)\n",
        "    if norm_type == float('inf'):\n",
        "        total_norm = max(p.grad.data.abs().max() for p in parameters)\n",
        "    else:\n",
        "        total_norm = 0\n",
        "        for p in parameters:\n",
        "            param_norm = p.grad.data.norm(norm_type)\n",
        "            total_norm += param_norm ** norm_type\n",
        "        total_norm = total_norm ** (1. / norm_type)\n",
        "    return total_norm\n",
        "\n",
        "def train(src_sents, tgt_sents, src_seqs, tgt_seqs, src_lens, tgt_lens,\n",
        "          encoder, decoder, encoder_optim, decoder_optim, opts):    \n",
        "    # -------------------------------------\n",
        "    # Prepare input and output placeholders\n",
        "    # -------------------------------------\n",
        "    # Last batch might not have the same size as we set to the `batch_size`\n",
        "    batch_size = src_seqs.size(1)\n",
        "    assert(batch_size == tgt_seqs.size(1))\n",
        "    \n",
        "    # Pack tensors to variables for neural network inputs (in order to autograd)\n",
        "    src_seqs = Variable(src_seqs)\n",
        "    tgt_seqs = Variable(tgt_seqs)\n",
        "    src_lens = Variable(torch.LongTensor(src_lens))\n",
        "    tgt_lens = Variable(torch.LongTensor(tgt_lens))\n",
        "\n",
        "    # Decoder's input\n",
        "    input_seq = Variable(torch.LongTensor([BOS] * batch_size))\n",
        "    \n",
        "    # Decoder's output sequence length = max target sequence length of current batch.\n",
        "    max_tgt_len = tgt_lens.data.max()\n",
        "    \n",
        "    \n",
        "    # Store all decoder's outputs.\n",
        "    # **CRUTIAL** \n",
        "    # Don't set:\n",
        "    # >> decoder_outputs = Variable(torch.zeros(max_tgt_len, batch_size, decoder.vocab_size))\n",
        "    # Varying tensor size could cause GPU allocate a new memory causing OOM, \n",
        "    # so we intialize tensor with fixed size instead:\n",
        "    # `opts.max_seq_len` is a fixed number, unlike `max_tgt_len` always varys.\n",
        "    decoder_outputs = Variable(torch.zeros(opts.max_seq_len, batch_size, decoder.vocab_size))\n",
        "\n",
        "    # Move variables from CPU to GPU.\n",
        "    if USE_CUDA:\n",
        "        src_seqs = src_seqs.cuda()\n",
        "        tgt_seqs = tgt_seqs.cuda()\n",
        "        src_lens = src_lens.cuda()\n",
        "        tgt_lens = tgt_lens.cuda()\n",
        "        input_seq = input_seq.cuda()\n",
        "        decoder_outputs = decoder_outputs.cuda()\n",
        "        \n",
        "    # -------------------------------------\n",
        "    # Training mode (enable dropout)\n",
        "    # -------------------------------------\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    # -------------------------------------\n",
        "    # Zero gradients, since optimizers will accumulate gradients for every backward.\n",
        "    # -------------------------------------\n",
        "    encoder_optim.zero_grad()\n",
        "    decoder_optim.zero_grad()\n",
        "        \n",
        "    # -------------------------------------\n",
        "    # Forward encoder\n",
        "    # -------------------------------------\n",
        "    encoder_outputs, encoder_hidden = encoder(src_seqs, src_lens.data.tolist())\n",
        "\n",
        "    # -------------------------------------\n",
        "    # Forward decoder\n",
        "    # -------------------------------------\n",
        "    # Initialize decoder's hidden state as encoder's last hidden state.\n",
        "    decoder_hidden = encoder_hidden\n",
        "    \n",
        "    # Run through decoder one time step at a time.\n",
        "    for t in range(max_tgt_len):\n",
        "        \n",
        "        # decoder returns:\n",
        "        # - decoder_output   : (batch_size, vocab_size)\n",
        "        # - decoder_hidden   : (num_layers, batch_size, hidden_size)\n",
        "        # - attention_weights: (batch_size, max_src_len)\n",
        "        decoder_output, decoder_hidden, attention_weights = decoder(input_seq, decoder_hidden,\n",
        "                                                                    encoder_outputs, src_lens)\n",
        "\n",
        "        # Store decoder outputs.\n",
        "        decoder_outputs[t] = decoder_output\n",
        "        \n",
        "        # Next input is current target\n",
        "        input_seq = tgt_seqs[t]\n",
        "        \n",
        "        # Detach hidden state:\n",
        "        detach_hidden(decoder_hidden)\n",
        "        \n",
        "    # -------------------------------------\n",
        "    # Compute loss\n",
        "    # -------------------------------------\n",
        "    loss, pred_seqs, num_corrects, num_words = masked_cross_entropy(\n",
        "        decoder_outputs[:max_tgt_len].transpose(0,1).contiguous(), \n",
        "        tgt_seqs.transpose(0,1).contiguous(),\n",
        "        tgt_lens\n",
        "    )\n",
        "    \n",
        "    pred_seqs = pred_seqs[:max_tgt_len]\n",
        "    \n",
        "    # -------------------------------------\n",
        "    # Backward and optimize\n",
        "    # -------------------------------------\n",
        "    # Backward to get gradients w.r.t parameters in model.\n",
        "    loss.backward()\n",
        "    \n",
        "    # Clip gradients\n",
        "    encoder_grad_norm = nn.utils.clip_grad_norm(encoder.parameters(), opts.max_grad_norm)\n",
        "    decoder_grad_norm = nn.utils.clip_grad_norm(decoder.parameters(), opts.max_grad_norm)\n",
        "    clipped_encoder_grad_norm = compute_grad_norm(encoder.parameters())\n",
        "    clipped_decoder_grad_norm = compute_grad_norm(decoder.parameters())\n",
        "    \n",
        "    # Update parameters with optimizers\n",
        "    encoder_optim.step()\n",
        "    decoder_optim.step()\n",
        "    print(\"========================================\")\n",
        "    print(loss)\n",
        "    print(loss.data)\n",
        "    print(\"========================================\")\n",
        "\n",
        "    return loss.data.item(), pred_seqs, attention_weights, num_corrects, num_words, encoder_grad_norm, decoder_grad_norm, clipped_encoder_grad_norm, clipped_decoder_grad_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hql8y_m2R-J",
        "outputId": "d03b776b-3c31-46e1-9288-98f53b760e67"
      },
      "source": [
        "train_dataset = QuestionAnswerDataset(src_path='validation_question.txt',\n",
        "                        tgt_path='validation_answer.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:00<00:00, 13187.04it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 15895.04it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 15694.31it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 15703.49it/s]\n",
            "230it [00:00, 298850.66it/s]\n",
            "100%|██████████| 234/234 [00:00<00:00, 108019.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Dataset preprocessing log:\n",
            "- Loading and tokenizing source sentences...\n",
            "- Loading and tokenizing target sentences...\n",
            "- Building source counter...\n",
            "- Building target counter...\n",
            "- Building source vocabulary...\n",
            "- Building target vocabulary...\n",
            "====================================================================================================\n",
            "Dataset Info:\n",
            "- Number of source sentences: 32\n",
            "- Number of target sentences: 32\n",
            "- Source vocabulary size: 234\n",
            "- Target vocabulary size: 234\n",
            "- Shared vocabulary: True\n",
            "====================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDkMb5a166aU",
        "outputId": "1e05a41e-0209-47f7-a709-4c017c8e0763"
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "train_iter = DataLoader(dataset=train_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=4,\n",
        "                        collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZxDa8My7Mqa"
      },
      "source": [
        "opts = AttrDict()\n",
        "\n",
        "# Configure models\n",
        "opts.word_vec_size = 300\n",
        "opts.rnn_type = 'LSTM'\n",
        "opts.hidden_size = 512\n",
        "opts.num_layers = 2\n",
        "opts.dropout = 0.3\n",
        "opts.bidirectional = True\n",
        "opts.attention = True\n",
        "opts.share_embeddings = True\n",
        "opts.pretrained_embeddings = True\n",
        "opts.fixed_embeddings = True\n",
        "opts.tie_embeddings = True # Tie decoder's input and output embeddings\n",
        "\n",
        "# Configure optimization\n",
        "opts.max_grad_norm = 2\n",
        "opts.learning_rate = 0.001\n",
        "opts.weight_decay = 1e-5 # L2 weight regularization\n",
        "\n",
        "# Configure training\n",
        "opts.max_seq_len = 100 # max sequence length to prevent OOM.\n",
        "opts.num_epochs = 5\n",
        "opts.print_every_step = 20\n",
        "opts.save_every_step = 5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyT5y8TD7WNC",
        "outputId": "64b3abdc-9f13-49da-eebc-e6caac4df7fe"
      },
      "source": [
        "for k,v in opts.items(): \n",
        "  print('- {}: {}'.format(k, v))\n",
        "print('='*100 + '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- word_vec_size: 300\n",
            "- rnn_type: LSTM\n",
            "- hidden_size: 512\n",
            "- num_layers: 2\n",
            "- dropout: 0.3\n",
            "- bidirectional: True\n",
            "- attention: True\n",
            "- share_embeddings: True\n",
            "- pretrained_embeddings: True\n",
            "- fixed_embeddings: True\n",
            "- tie_embeddings: True\n",
            "- max_grad_norm: 2\n",
            "- learning_rate: 0.001\n",
            "- weight_decay: 1e-05\n",
            "- max_seq_len: 100\n",
            "- num_epochs: 5\n",
            "- print_every_step: 20\n",
            "- save_every_step: 5000\n",
            "====================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLLPiBoL7hg_"
      },
      "source": [
        "src_vocab_size = len(train_dataset.src_vocab.token2id)\n",
        "tgt_vocab_size = len(train_dataset.tgt_vocab.token2id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD7uRIScOtRl",
        "outputId": "1c462e92-e607-4ceb-b8e8-caefe8829bce"
      },
      "source": [
        "nlp = spacy.load('en_core_web_lg')\n",
        "print(nlp.vocab.vectors_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1neK-Y_R7iye",
        "outputId": "db69b357-be74-4fb9-c4b3-48445022e9d1"
      },
      "source": [
        "print(nlp.vocab.vectors_length)\n",
        "word_vec_size = opts.word_vec_size if not opts.pretrained_embeddings else nlp.vocab.vectors_length\n",
        "print(word_vec_size)\n",
        "src_embedding = nn.Embedding(src_vocab_size, word_vec_size, padding_idx=PAD)\n",
        "tgt_embedding = nn.Embedding(tgt_vocab_size, word_vec_size, padding_idx=PAD)\n",
        "\n",
        "if opts.share_embeddings:\n",
        "    assert(src_vocab_size == tgt_vocab_size)\n",
        "    tgt_embedding.weight = src_embedding.weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltYQ_l9N85Hj",
        "outputId": "cb6a9bef-96e9-4504-d88b-62906981938c"
      },
      "source": [
        "encoder = EncoderRNN(embedding=src_embedding,\n",
        "                     rnn_type=opts.rnn_type,\n",
        "                     hidden_size=opts.hidden_size,\n",
        "                     num_layers=opts.num_layers,\n",
        "                     dropout=opts.dropout,\n",
        "                     bidirectional=opts.bidirectional)\n",
        "\n",
        "decoder = LuongAttnDecoderRNN(encoder, embedding=tgt_embedding,\n",
        "                              attention=opts.attention,\n",
        "                              tie_embeddings=opts.tie_embeddings,\n",
        "                              dropout=opts.dropout)\n",
        "\n",
        "if opts.pretrained_embeddings:\n",
        "    glove_embeddings = load_spacy_glove_embedding(nlp, train_dataset.src_vocab)\n",
        "    encoder.embedding.weight.data.copy_(glove_embeddings)\n",
        "    decoder.embedding.weight.data.copy_(glove_embeddings)\n",
        "    if opts.fixed_embeddings:\n",
        "        encoder.embedding.weight.requires_grad = False\n",
        "        decoder.embedding.weight.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:00<00:00, 20039.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Loading spacy glove embedding:\n",
            "- Vocabulary size: 234\n",
            "- Word vector size: 300\n",
            "- Unknown word count: 45\n",
            "====================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObbRmXPxDvg8"
      },
      "source": [
        "if True:\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DbrGZAsD_Z1",
        "outputId": "eeb1bc23-d9cb-4f95-85c6-4be5f402f7c6"
      },
      "source": [
        "print('='*100)\n",
        "print('Model log:\\n')\n",
        "print(encoder)\n",
        "print(decoder)\n",
        "print('- Encoder input embedding requires_grad={}'.format(encoder.embedding.weight.requires_grad))\n",
        "print('- Decoder input embedding requires_grad={}'.format(decoder.embedding.weight.requires_grad))\n",
        "print('- Decoder output embedding requires_grad={}'.format(decoder.W_s.weight.requires_grad))\n",
        "print('='*100 + '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Model log:\n",
            "\n",
            "EncoderRNN(\n",
            "  (embedding): Embedding(234, 300, padding_idx=0)\n",
            "  (rnn): LSTM(300, 256, num_layers=2, dropout=0.3, bidirectional=True)\n",
            ")\n",
            "LuongAttnDecoderRNN(\n",
            "  (embedding): Embedding(234, 300, padding_idx=0)\n",
            "  (rnn): LSTM(300, 512, num_layers=2, dropout=0.3)\n",
            "  (W_a): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (W_c): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (W_proj): Linear(in_features=512, out_features=300, bias=True)\n",
            "  (W_s): Linear(in_features=300, out_features=234, bias=True)\n",
            ")\n",
            "- Encoder input embedding requires_grad=False\n",
            "- Decoder input embedding requires_grad=False\n",
            "- Decoder output embedding requires_grad=False\n",
            "====================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ADlo7sIEEf7"
      },
      "source": [
        "encoder_optim = optim.Adam([p for p in encoder.parameters() if p.requires_grad], lr=opts.learning_rate, weight_decay=opts.weight_decay)\n",
        "decoder_optim = optim.Adam([p for p in decoder.parameters() if p.requires_grad], lr=opts.learning_rate, weight_decay=opts.weight_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrMZ3YA-ExTR"
      },
      "source": [
        "USE_CUDA = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaW6keSIEPuB",
        "outputId": "05b70aa6-7bfe-4fcf-83c8-9a43f2fcc9e0"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (54.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvAJ9GTMEF3O"
      },
      "source": [
        "from datetime import datetime\n",
        "from tensorboardX import SummaryWriter\n",
        "# --------------------------\n",
        "# Configure tensorboard\n",
        "# --------------------------\n",
        "model_name = 'seq2seq'\n",
        "datetime = ('%s' % datetime.now()).split('.')[0]\n",
        "experiment_name = '{}_{}'.format(model_name, datetime)\n",
        "tensorboard_log_dir = './tensorboard-logs/{}/'.format(experiment_name)\n",
        "writer = SummaryWriter(tensorboard_log_dir)\n",
        "\n",
        "# --------------------------\n",
        "# Configure training\n",
        "# --------------------------\n",
        "num_epochs = opts.num_epochs\n",
        "print_every_step = opts.print_every_step\n",
        "save_every_step = opts.save_every_step\n",
        "# For saving checkpoint and tensorboard\n",
        "global_step = 0\n",
        "\n",
        "# --------------------------\n",
        "# Start training\n",
        "# --------------------------\n",
        "total_loss = 0\n",
        "total_corrects = 0\n",
        "total_words = 0\n",
        "prev_gpu_memory_usage = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxKhcOrdEZ_A",
        "outputId": "5e737803-68d3-4155-8ce5-76a3efa7866e"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for batch_id, batch_data in tqdm(enumerate(train_iter)):\n",
        "\n",
        "        # Unpack batch data\n",
        "        src_sents, tgt_sents, src_seqs, tgt_seqs, src_lens, tgt_lens = batch_data\n",
        "        \n",
        "        # Ignore batch if there is a long sequence.\n",
        "        max_seq_len = max(src_lens + tgt_lens)\n",
        "        if max_seq_len > opts.max_seq_len:\n",
        "            print('[!] Ignore batch: sequence length={} > max sequence length={}'.format(max_seq_len, opts.max_seq_len))\n",
        "            continue\n",
        "        \n",
        "        # Train.\n",
        "        loss, pred_seqs, attention_weights, num_corrects, num_words, encoder_grad_norm, decoder_grad_norm, clipped_encoder_grad_norm, clipped_decoder_grad_norm = train(src_sents, tgt_sents, src_seqs, tgt_seqs, src_lens, tgt_lens, encoder, decoder, encoder_optim, decoder_optim, opts)\n",
        "\n",
        "        # Statistics.\n",
        "        global_step += 1\n",
        "        total_loss += loss\n",
        "        total_corrects += num_corrects\n",
        "        total_words += num_words\n",
        "        total_accuracy = 100 * (total_corrects / total_words)\n",
        "        \n",
        "        # Save checkpoint.\n",
        "        if global_step % save_every_step == 0:\n",
        "            \n",
        "            checkpoint_path = save_checkpoint(opts, experiment_name, encoder, decoder, encoder_optim, decoder_optim, \n",
        "                                              total_accuracy, total_loss, global_step)\n",
        "            \n",
        "            print('='*100)\n",
        "            print('Save checkpoint to \"{}\".'.format(checkpoint_path))\n",
        "            print('='*100 + '\\n')\n",
        "\n",
        "        # Print statistics and write to Tensorboard.\n",
        "        if global_step % print_every_step == 0:\n",
        "            \n",
        "            curr_gpu_memory_usage = get_gpu_memory_usage(device_id=torch.cuda.current_device())\n",
        "            diff_gpu_memory_usage = curr_gpu_memory_usage - prev_gpu_memory_usage\n",
        "            prev_gpu_memory_usage = curr_gpu_memory_usage\n",
        "            \n",
        "            print('='*100)\n",
        "            print('Training log:')\n",
        "            print('- Epoch: {}/{}'.format(epoch, num_epochs))\n",
        "            print('- Global step: {}'.format(global_step))\n",
        "            print('- Total loss: {}'.format(total_loss))\n",
        "            print('- Total corrects: {}'.format(total_corrects))\n",
        "            print('- Total words: {}'.format(total_words))\n",
        "            print('- Total accuracy: {}'.format(total_accuracy))\n",
        "            print('- Current GPU memory usage: {}'.format(curr_gpu_memory_usage))\n",
        "            print('- Diff GPU memory usage: {}'.format(diff_gpu_memory_usage))\n",
        "            print('='*100 + '\\n')\n",
        "            \n",
        "            write_to_tensorboard(writer, global_step, total_loss, total_corrects, total_words, total_accuracy,\n",
        "                                 encoder_grad_norm, decoder_grad_norm, clipped_encoder_grad_norm, clipped_decoder_grad_norm,\n",
        "                                 encoder, decoder,\n",
        "                                 gpu_memory_usage={\n",
        "                                     'curr': curr_gpu_memory_usage,\n",
        "                                     'diff': diff_gpu_memory_usage\n",
        "                                 })\n",
        "            \n",
        "            total_loss = 0\n",
        "            total_corrects = 0\n",
        "            total_words = 0\n",
        "\n",
        "        # Free memory\n",
        "        del src_sents, tgt_sents, src_seqs, tgt_seqs, src_lens, tgt_lens, \\\n",
        "            loss, pred_seqs, attention_weights, num_corrects, num_words, \\\n",
        "            encoder_grad_norm, decoder_grad_norm, clipped_encoder_grad_norm, clipped_decoder_grad_norm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:116: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:117: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "5it [00:00, 18.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "tensor(5.4515, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.4515, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(5.1986, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.1986, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(6.4593, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.4593, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(5.0672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0672, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(5.2037, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2037, device='cuda:0')\n",
            "========================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8it [00:00, 16.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "tensor(5.3319, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3319, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.9483, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9483, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(5.1329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.1329, device='cuda:0')\n",
            "========================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "6it [00:00, 23.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "tensor(4.7550, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7550, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(5.0139, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0139, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.8480, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.8480, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.8241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.8241, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.9052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9052, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.6487, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6487, device='cuda:0')\n",
            "========================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r8it [00:00, 18.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "tensor(4.6163, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6163, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(5.2050, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2050, device='cuda:0')\n",
            "========================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "3it [00:00, 22.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "tensor(4.5692, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.5692, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.5915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.5915, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.4263, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.4263, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.5725, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.5725, device='cuda:0')\n",
            "========================================\n",
            "====================================================================================================\n",
            "Training log:\n",
            "- Epoch: 2/5\n",
            "- Global step: 20\n",
            "- Total loss: 99.76907682418823\n",
            "- Total corrects: 84\n",
            "- Total words: 802\n",
            "- Total accuracy: 10.47381591796875\n",
            "- Current GPU memory usage: 1258\n",
            "- Diff GPU memory usage: 1258\n",
            "====================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4it [00:02,  1.36it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:116: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:117: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "7it [00:02,  1.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "tensor(4.4472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.4472, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.3395, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3395, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.6476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6476, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.0346, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0346, device='cuda:0')\n",
            "========================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r8it [00:02,  2.90it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "5it [00:00, 19.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "tensor(4.3093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3093, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0511, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.3569, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3569, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.0746, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0746, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.3835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3835, device='cuda:0')\n",
            "========================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8it [00:00, 18.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "tensor(3.7684, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.7684, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.1327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1327, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(3.8324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8324, device='cuda:0')\n",
            "========================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "3it [00:00, 20.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "tensor(3.7773, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.7773, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.1713, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1713, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(3.8052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8052, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.1475, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1475, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(3.5263, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5263, device='cuda:0')\n",
            "========================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r6it [00:00, 21.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "tensor(3.2991, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.2991, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(3.2151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.2151, device='cuda:0')\n",
            "========================================\n",
            "========================================\n",
            "tensor(4.3218, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3218, device='cuda:0')\n",
            "========================================\n",
            "====================================================================================================\n",
            "Training log:\n",
            "- Epoch: 4/5\n",
            "- Global step: 40\n",
            "- Total loss: 80.64121174812317\n",
            "- Total corrects: 133\n",
            "- Total words: 768\n",
            "- Total accuracy: 17.31770896911621\n",
            "- Current GPU memory usage: 1258\n",
            "- Diff GPU memory usage: 0\n",
            "====================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8it [00:02,  2.86it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTnWlQ_vPR3N",
        "outputId": "88560cd0-1e63-424e-fbe2-36ffa97d0699"
      },
      "source": [
        "checkpoint_path = save_checkpoint(opts, experiment_name, encoder, decoder, encoder_optim, decoder_optim, \n",
        "                                              total_accuracy, total_loss, global_step)\n",
        "            \n",
        "print('='*100)\n",
        "print('Save checkpoint to \"{}\".'.format(checkpoint_path))\n",
        "print('='*100 + '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Save checkpoint to \"checkpoints/seq2seq_2021-03-20 10:44:03_acc_17.32_loss_0.00_step_40.pt\".\n",
            "====================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRsbsIQ5Pad0"
      },
      "source": [
        "def translate(src_text, train_dataset, encoder, decoder, max_seq_len, replace_unk=True):\n",
        "    # -------------------------------------\n",
        "    # Prepare input and output placeholders\n",
        "    # -------------------------------------\n",
        "    # Like dataset's `__getitem__()` and dataloader's `collate_fn()`.\n",
        "    src_sent = src_text.split()\n",
        "    src_seqs = torch.LongTensor([train_dataset.tokens2ids(tokens=src_text.split(),\n",
        "                                                          token2id=train_dataset.src_vocab.token2id,\n",
        "                                                          append_BOS=False, append_EOS=True)]).transpose(0,1)\n",
        "    src_lens = [len(src_seqs)]\n",
        "    \n",
        "    # Last batch might not have the same size as we set to the `batch_size`\n",
        "    batch_size = src_seqs.size(1)\n",
        "    \n",
        "    # Pack tensors to variables for neural network inputs (in order to autograd)\n",
        "    src_seqs = Variable(src_seqs, volatile=True)\n",
        "    src_lens = Variable(torch.LongTensor(src_lens), volatile=True)\n",
        "\n",
        "    # Decoder's input\n",
        "    input_seq = Variable(torch.LongTensor([BOS] * batch_size), volatile=True)\n",
        "    # Store output words and attention states\n",
        "    out_sent = []\n",
        "    all_attention_weights = torch.zeros(max_seq_len, len(src_seqs))\n",
        "    \n",
        "    # Move variables from CPU to GPU.\n",
        "    if USE_CUDA:\n",
        "        src_seqs = src_seqs.cuda()\n",
        "        src_lens = src_lens.cuda()\n",
        "        input_seq = input_seq.cuda()\n",
        "        \n",
        "    # -------------------------------------\n",
        "    # Evaluation mode (disable dropout)\n",
        "    # -------------------------------------\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "        \n",
        "    # -------------------------------------\n",
        "    # Forward encoder\n",
        "    # -------------------------------------\n",
        "    encoder_outputs, encoder_hidden = encoder(src_seqs, src_lens.data.tolist())\n",
        "\n",
        "    # -------------------------------------\n",
        "    # Forward decoder\n",
        "    # -------------------------------------\n",
        "    # Initialize decoder's hidden state as encoder's last hidden state.\n",
        "    decoder_hidden = encoder_hidden\n",
        "    \n",
        "    # Run through decoder one time step at a time.\n",
        "    for t in range(max_seq_len):\n",
        "        \n",
        "        # decoder returns:\n",
        "        # - decoder_output   : (batch_size, vocab_size)\n",
        "        # - decoder_hidden   : (num_layers, batch_size, hidden_size)\n",
        "        # - attention_weights: (batch_size, max_src_len)\n",
        "        decoder_output, decoder_hidden, attention_weights = decoder(input_seq, decoder_hidden,\n",
        "                                                                    encoder_outputs, src_lens)\n",
        "\n",
        "        # Store attention weights.\n",
        "        # .squeeze(0): remove `batch_size` dimension since batch_size=1\n",
        "        all_attention_weights[t] = attention_weights.squeeze(0).cpu().data \n",
        "        \n",
        "        # Choose top word from decoder's output\n",
        "        prob, token_id = decoder_output.data.topk(1)\n",
        "        token_id = token_id[0][0] # get value\n",
        "        if token_id == EOS:\n",
        "            break\n",
        "        else:\n",
        "            if token_id == UNK and replace_unk:\n",
        "                # Replace unk by selecting the source token with the highest attention score.\n",
        "                score, idx = all_attention_weights[t].max(0)\n",
        "                token = src_sent[idx[0]]\n",
        "            else:\n",
        "                # <UNK>\n",
        "                token = train_dataset.tgt_vocab.id2token[token_id.item()]\n",
        "            \n",
        "            out_sent.append(token)\n",
        "        \n",
        "        # Next input is chosen word\n",
        "        input_seq = Variable(torch.LongTensor([token_id]), volatile=True)\n",
        "        if USE_CUDA: input_seq = input_seq.cuda()\n",
        "            \n",
        "        # Repackage hidden state (may not need this, since no BPTT)\n",
        "        detach_hidden(decoder_hidden)\n",
        "    \n",
        "    src_text = ' '.join([train_dataset.src_vocab.id2token[token_id] for token_id in src_seqs.data.squeeze(1).tolist()])\n",
        "    out_text = ' '.join(out_sent)\n",
        "        \n",
        "    # all_attention_weights: (out_len, src_len)\n",
        "    return src_text, out_text, all_attention_weights[:len(out_sent)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrRLt1z2Pcgy",
        "outputId": "bf38f2b6-85df-427d-ef41-e82d5a901100"
      },
      "source": [
        "src_text, out_text, all_attention_weights = translate('who is Alec Cobbe ?', train_dataset, encoder, decoder, max_seq_len=opts.max_seq_len)\n",
        "src_text, out_text, all_attention_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:79: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('who is Alec Cobbe <UNK> <EOS>',\n",
              " 'Wilbur Wilbur of Wilbur',\n",
              " tensor([[0.2529, 0.2408, 0.2182, 0.1324, 0.0881, 0.0676],\n",
              "         [0.2827, 0.2282, 0.1867, 0.1189, 0.0941, 0.0895],\n",
              "         [0.2963, 0.2130, 0.1626, 0.1107, 0.1021, 0.1154],\n",
              "         [0.2643, 0.2269, 0.1843, 0.1232, 0.1015, 0.0999]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    }
  ]
}